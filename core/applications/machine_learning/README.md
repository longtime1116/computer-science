- [x] week1(2018-12-04〜2018-12-10)
  - 教師あり学習(Supervised Learning)
    - 正解を与えて予測させる
    - 回帰(Regression): 連続的な数値を予想する(株価とか)
    - 分類(Classification): 離散的な数字に分類する(true/falseなど)
  - 教師なし学習(Unsupervised Learning)
    - 正解を与えずにクラスタリングする
    - 大量のニュース記事を同じ内容のニュースごとに分けるなど
  - 線形回帰(教師あり学習のモデルのひとつ)
    - Traning Set から Learning Algorithm により関数 h (hypothesis の h) を生み出す。
    - この関数は例えば家のサイズを input としてその販売価格を output とするもの
    - h が直線の時(線形関数に当てはめる時)、それを線形回帰と呼ぶ
  - 目的関数(Cost Function)
    - 線形回帰のモデルの正確さは、目的関数(例えば二乗誤差目的関数)で測ることができる。
    - 目的関数の output が最小になるときのものが、よい仮設関数である。
    - 二乗誤差を使うのは、基本的にそれが一番よくフィットすることが多いから、という理由のようだ
    - 等高線図(Contour plot)という概念
  - 最急降下法(Gradient Descent)
    - 二次元平面の例で直感的に捉えると、勾配の最も急な方向に対して一歩ずつ進んでいき高さが極小になるところまで進んだら満足する、というようなアルゴリズム。
    - θ0 とθ1 は同時に更新していく(同時更新)
- [x] week2(2018-12-11〜2018-12-22)
  - multiple features を似たような scale にすると gradient descent しやすくなる。
    - -1 <= xi <= 1 くらいになっていると良い。が、完全に同じ scale でなければならないわけではない
  - gradient descent がうまく効いていれば、イテレーションごとに誤差は減っていく。どれくらいのイテレーションで収束するかは事前には不明。
  - 減る誤差の値の最小値 epsilon を定義しておき、それよりも現象幅が短くなったら終了する、みたいに自動でやることができる。
  - learning rate(α)が十分に小さければ、ちゃんと収束する。が、小さければ小さいほど、イテレーション回数が増えてしまう。
    - αは0.001, 0.01, 0.1, 1 と色々試してみて良さそうなのを決める。
  - gradient descent はαを選びイテレートしなければならない一方、normal equation を解く方法ならば、αもいらないし一発で出る。
    - ただし、normal equation を特には pinv(x'x) を解かなければならず、これは O(n^3) なので、n が大きいと辛い
    - x'x が正則でない場合、pinv ではなく inv を使うらしい。
    - 正則ではない主なケースとしては、以下。
      - redundant な特徴がある。(メートル記法の結果とfeet記法の結果がそれぞれあったら、そりゃ正則にはならない)
      - サンプル数より特徴量の方が多いケース
- [x] week3(2018-12-25〜2018-12-29)
  - 分類(Classification)
  - 線形回帰を分類に適用することもできるが、分類には明らかに不必要なデータが仮説に影響を与えてしまうことがあるので、精度が悪くなる可能性があり望ましくない。
  - ロジスティック回帰
  - シグモイド関数(== ロジスティック関数)
    - h = g(z)
    - z = θ'x'
    - g(z) = 1 / (1 + e^(-z))
  - decision boundary(決定境界)
  - Regularized Linear Regression(正則化回帰) により、overfitting を解消しつつ、m < n のケースにも対応できる。
- [x] week4(2018-12-31〜2019-01-04)
  - Neural Networks
  - forward propagation によって進んでいき最後に output にいきつく
  - 例として、XNOR を作る Newral Network
- [x] week5(2019-01-26〜2019-02-03)
  - Backpropagation(誤差逆伝播法)
- [x] week6(2019-02-05〜2019-02-15)
  - Deep Learning を使って問題を解決するとき、次の一手として何をすべきか？をどう決定すべきか
- [x] week7(2019-02-16〜2019-02-25)
  - SVM はマージン最大化が嬉しい
- [x] week8(2019-02-27〜2019-03-04)
  - 教師なし学習
  - K-means Clustering で分類
  - PCA で次元圧縮することで計算リソースや速度を改善できる
- [x] week9(2019-03-05〜2019-03-20)
  - アノマリー検出
    - アノマリーとは、変則的なもの
  - ガウス分布(正規分布) μ(平均)とσ^2(分散)で形が決まる
- [x] week10(2019-03-21〜2019-03-21)
  - stochastic gradient descent, mini batch, batch
  - online learning
  - map-reduce
- [x] week11(2019-03-23〜2019-03-23)
  - pipeline の考え方を Photo OCR を題材に説明。
  - sliding window で判定
  - 既存のデータから、工夫することでデータ数を増やすことができる
  - ceiling analysys でボトルネックを見つけて、そこにリソースを投下すべき

- 感想
  - 数学のような厳密な論理の積み重ねを大切にする世界とは全く違う世界だ。
  - 機械学習の世界からは、「このようなモデルを仮定したらいい感じに動いたんだけど君も使ったら？」みたいな匂いを感じた。
  - 直感からモデルを思いつき、それを現実に当てはめて良い感じだったら採用！みたいな。いかにも実学という雰囲気。
  - 学生時代に純粋数学の世界に首を突っ込んでいた自分としては、そこに気づいて発想を転換してからは理解するスピードがガッと上がった。
  - なぜこのコスト関数を使うのか？に深い意味はない(意味はあるが、少なくともそこに"正しさ"はない)。
  - なので「なんでこれがいきなり出てきたんだ！？」とか考えることに必要はない。どんな特徴があるからこれを使うことに意味があるのか、は考えるに値すると思うけれど。
  - ちなみにこの感じ、学生時代に経済学を学んだときにも似たような印象を持った。
  - 初歩的なミクロ経済学では、経済的合理人や効用関数を便宜的に定義して、その前提のもとで議論を進めたりする。絶対的な真理からではなく仮定から始まるという点で似ている。
  - 自分はそこに美しさを感じられず、純粋数学の方面に引き寄せられていったのだが。。。
  - ということで、機械学習がもたらす価値については興味深いと感じるし、随所にテクニカルな面白みを感じるものの、美しい学問だとはあまり思えないなぁ、と感じている。
  - (とはいえ今後も学びはする。)
