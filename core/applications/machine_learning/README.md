- [x] week1(2018-12-04〜2018-12-10)
  - 教師あり学習(Supervised Learning)
    - 正解を与えて予測させる
    - 回帰(Regression): 連続的な数値を予想する(株価とか)
    - 分類(Classification): 離散的な数字に分類する(true/falseなど)
  - 教師なし学習(Unsupervised Learning)
    - 正解を与えずにクラスタリングする
    - 大量のニュース記事を同じ内容のニュースごとに分けるなど
  - 線形回帰(教師あり学習のモデルのひとつ)
    - Traning Set から Learning Algorithm により関数 h (hypothesis の h) を生み出す。
    - この関数は例えば家のサイズを input としてその販売価格を output とするもの
    - h が直線の時(線形関数に当てはめる時)、それを線形回帰と呼ぶ
  - 目的関数(Cost Function)
    - 線形回帰のモデルの正確さは、目的関数(例えば二乗誤差目的関数)で測ることができる。
    - 目的関数の output が最小になるときのものが、よい仮設関数である。
    - 二乗誤差を使うのは、基本的にそれが一番よくフィットすることが多いから、という理由のようだ
    - 等高線図(Contour plot)という概念
  - 最急降下法(Gradient Descent)
    - 二次元平面の例で直感的に捉えると、勾配の最も急な方向に対して一歩ずつ進んでいき高さが極小になるところまで進んだら満足する、というようなアルゴリズム。
    - θ0 とθ1 は同時に更新していく(同時更新)
- [x] week2(2018-12-11〜2018-12-22)
  - multiple features を似たような scale にすると gradient descent しやすくなる。
    - -1 <= xi <= 1 くらいになっていると良い。が、完全に同じ scale でなければならないわけではない
  - gradient descent がうまく効いていれば、イテレーションごとに誤差は減っていく。どれくらいのイテレーションで収束するかは事前には不明。
  - 減る誤差の値の最小値 epsilon を定義しておき、それよりも現象幅が短くなったら終了する、みたいに自動でやることができる。
  - learning rate(α)が十分に小さければ、ちゃんと収束する。が、小さければ小さいほど、イテレーション回数が増えてしまう。
    - αは0.001, 0.01, 0.1, 1 と色々試してみて良さそうなのを決める。
  - gradient descent はαを選びイテレートしなければならない一方、normal equation を解く方法ならば、αもいらないし一発で出る。
    - ただし、normal equation を特には pinv(x'x) を解かなければならず、これは O(n^3) なので、n が大きいと辛い
    - x'x が正則でない場合、pinv ではなく inv を使うらしい。
    - 正則ではない主なケースとしては、以下。
      - redundant な特徴がある。(メートル記法の結果とfeet記法の結果がそれぞれあったら、そりゃ正則にはならない)
      - サンプル数より特徴量の方が多いケース
- [x] week3(2018-12-25〜2018-12-29)
  - 分類(Classification)
  - 線形回帰を分類に適用することもできるが、分類には明らかに不必要なデータが仮説に影響を与えてしまうことがあるので、精度が悪くなる可能性があり望ましくない。
  - ロジスティック回帰
  - シグモイド関数(== ロジスティック関数)
    - h = g(z)
    - z = θ'x'
    - g(z) = 1 / (1 + e^(-z))
  - decision boundary(決定境界)
  - Regularized Linear Regression(正則化回帰) により、overfitting を解消しつつ、m < n のケースにも対応できる。
- [x] week4(2018-12-31〜2019-01-04)
  - Neural Networks
  - forward propagation によって進んでいき最後に output にいきつく
  - 例として、XNOR を作る Newral Network
- [x] week5(2019-01-26〜2019-02-03)
  - Backpropagation(誤差逆伝播法)
- [x] week6(2019-02-05〜2019-02-15)
  - Deep Learning を使って問題を解決するとき、次の一手として何をすべきか？をどう決定すべきか
- [x] week7(2019-02-16〜2019-02-25)
  - SVM はマージン最大化が嬉しい
- [x] week8(2019-02-27〜2019-03-04)
  - 教師なし学習
  - K-means Clustering で分類
  - PCA で次元圧縮することで計算リソースや速度を改善できる
- [x] week9(2019-03-05〜2019-03-20)
  - アノマリー検出
    - アノマリーとは、変則的なもの
  - ガウス分布(正規分布) μ(平均)とσ^2(分散)で形が決まる
- [x] week10(2019-03-21〜2019-03-21)
  - stochastic gradient descent, mini batch, batch
  - online learning
  - map-reduce
- [] week11(2019-03-23〜)
